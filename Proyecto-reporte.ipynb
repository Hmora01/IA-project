{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e534ca0a",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Exploration on `CartPole-v1` by [GitHub User]\n",
    "\n",
    "This report highlights the remarkable work conducted by a GitHub user on applying reinforcement learning techniques to solve the `CartPole-v1` environment using the Policy Gradients method. The project showcases the development from a basic policy to a sophisticated neural network-based strategy.\n",
    "\n",
    "## Initial Strategy: Basic Policy\n",
    "\n",
    "The initial approach employed a simple, intuitive policy: if the pole tilts left, the cart moves left, and vice versa. This strategy, although straightforward, managed to achieve a maximum of 63 steps in keeping the pole balanced, underscoring its limitations in solving the environment completely, which requires the pole to be balanced for 200 steps.\n",
    "\n",
    "## Advancement with Neural Network Policies\n",
    "\n",
    "Recognizing the need for a more advanced solution, the GitHub user introduced a neural network to function as the policy model. This model predicts the probability of moving the cart left or right based on the environment's state, aiming to enhance decision-making.\n",
    "\n",
    "### Neural Network Design:\n",
    "\n",
    "- **Input Layer**: Takes in the state of the environment.\n",
    "- **Hidden Layer**: Comprises 5 neurons with ReLU activation to introduce complexity.\n",
    "- **Output Layer**: A single neuron with sigmoid activation outputs the probability of moving left.\n",
    "\n",
    "## Training via Policy Gradients\n",
    "\n",
    "The Policy Gradients algorithm was utilized for training the neural network. This involved:\n",
    "\n",
    "- **Episode Playthroughs**: Generating data by playing multiple episodes.\n",
    "- **Reward Discounting**: Valuing immediate over delayed rewards to tackle the credit assignment problem.\n",
    "- **Reward Normalization**: Standardizing the learning signal across episodes.\n",
    "- **Probability Adjustment**: Making beneficial actions more probable based on their outcomes.\n",
    "\n",
    "## Achievements\n",
    "\n",
    "Through iterative training (150 iterations with 10 episodes each), the neural network policy demonstrated significant progress:\n",
    "\n",
    "- **Mean Rewards**: The policy notably reached an average of nearly 190.3 rewards per episode, showcasing its ability to nearly solve the `CartPole-v1` environment consistently.\n",
    "\n",
    "## Conclusion and Acknowledgement\n",
    "\n",
    "The GitHub user's exploration into reinforcement learning with Policy Gradients has effectively demonstrated how a shift from basic strategies to more complex neural network policies can significantly improve performance in the `CartPole-v1` environment. This work not only highlights the potential of neural networks in learning and decision-making tasks but also serves as an insightful reference for those looking to delve into reinforcement learning.\n",
    "\n",
    "We commend the original contributor for their innovative approach and contributions to the field.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f2f3e21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 21.2     |\n",
      "|    ep_rew_mean     | 21.2     |\n",
      "| time/              |          |\n",
      "|    fps             | 5318     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 0        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 26.1       |\n",
      "|    ep_rew_mean          | 26.1       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 3467       |\n",
      "|    iterations           | 2          |\n",
      "|    time_elapsed         | 1          |\n",
      "|    total_timesteps      | 4096       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00899041 |\n",
      "|    clip_fraction        | 0.107      |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.686     |\n",
      "|    explained_variance   | -0.00108   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 7.19       |\n",
      "|    n_updates            | 10         |\n",
      "|    policy_gradient_loss | -0.0162    |\n",
      "|    value_loss           | 49.8       |\n",
      "----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 36          |\n",
      "|    ep_rew_mean          | 36          |\n",
      "| time/                   |             |\n",
      "|    fps                  | 3163        |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 1           |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009659998 |\n",
      "|    clip_fraction        | 0.0586      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.663      |\n",
      "|    explained_variance   | 0.114       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.7        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.017      |\n",
      "|    value_loss           | 39.6        |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 46.6       |\n",
      "|    ep_rew_mean          | 46.6       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 3014       |\n",
      "|    iterations           | 4          |\n",
      "|    time_elapsed         | 2          |\n",
      "|    total_timesteps      | 8192       |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.00839093 |\n",
      "|    clip_fraction        | 0.0785     |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -0.634     |\n",
      "|    explained_variance   | 0.23       |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 20.9       |\n",
      "|    n_updates            | 30         |\n",
      "|    policy_gradient_loss | -0.0182    |\n",
      "|    value_loss           | 54.4       |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 63.6         |\n",
      "|    ep_rew_mean          | 63.6         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 2943         |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 3            |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0140310405 |\n",
      "|    clip_fraction        | 0.106        |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.604       |\n",
      "|    explained_variance   | 0.388        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 35           |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.0209      |\n",
      "|    value_loss           | 60.7         |\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Create the environment\n",
    "env = make_vec_env('CartPole-v1', n_envs=1)\n",
    "\n",
    "# Initialize the agent\n",
    "model = PPO('MlpPolicy', env, verbose=1)\n",
    "\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=10000)\n",
    "\n",
    "# Evaluate the agent\n",
    "obs = env.reset()\n",
    "for i in range(1000):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    env.render()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4a3afef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 0.04134518, -0.01705906, -0.01273812, -0.03185254], dtype=float32), {})\n"
     ]
    }
   ],
   "source": [
    "initial_state = env.reset()\n",
    "print(initial_state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e3ecc5",
   "metadata": {},
   "source": [
    "# PPO Agent Training on CartPole-v1 Environment\n",
    "\n",
    "In this project, we employed the Proximal Policy Optimization (PPO) algorithm, a state-of-the-art reinforcement learning method, to train an agent on the `CartPole-v1` environment. This classic environment challenges the agent to balance a pole on a moving cart, serving as a benchmark for evaluating the performance of reinforcement learning algorithms.\n",
    "\n",
    "## Environment Setup and Agent Initialization\n",
    "\n",
    "The environment was initialized using the `make_vec_env` function from the Stable Baselines3 library, allowing for straightforward setup and potential parallelization. For this demonstration, a single environment instance was used:\n",
    "\n",
    "```python\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Create the environment\n",
    "env = make_vec_env('CartPole-v1', n_envs=1)\n",
    "\n",
    "Following this, we initialized the PPO agent with a Multi-Layer Perceptron (MLP) policy and set the verbosity level to 1 to enable detailed logging throughout the training process:\n",
    "    \n",
    "# Initialize the agent\n",
    "model = PPO('MlpPolicy', env, verbose=1)\n",
    "\n",
    "\n",
    "# Training\n",
    "The agent was trained over 10,000 timesteps, engaging with the environment, gathering experiences, and iteratively refining its policy based on the rewards and outcomes of its actions.\n",
    "\n",
    "## Training Results\n",
    "Throughout the training phase, several key metrics were logged, shedding light on the agent's performance and the learning progression. These metrics included the average episode length, average rewards, and various loss metrics essential for understanding the PPO algorithm's optimization dynamics.\n",
    "\n",
    "# A brief overview of the training progression based on the logs:\n",
    "\n",
    "There was a consistent increase in the average length of episodes and the average rewards, indicating the agent's successful learning to keep the pole balanced for longer periods.\n",
    "Performance indicators such as frames per second (fps) and total timesteps were monitored, providing insight into the training process's efficiency.\n",
    "The monitoring of loss metrics (e.g., policy gradient loss, value loss) ensured the stability and effectiveness of the learning algorithm.\n",
    "For instance, towards the end of training:\n",
    "\n",
    "The average episode length improved to about 63.6 steps.\n",
    "The average reward per episode also increased, reflecting the agent's enhanced ability to maintain the pole's balance.\n",
    "Evaluation and Visualization\n",
    "Post-training, the agent's performance was evaluated through its interaction with the environment, employing the learned policy. Although direct visualization results are not displayed in this report, they can be achieved by running the provided code within an environment that supports rendering, enabling qualitative assessment of the agent's behavior.\n",
    "\n",
    "## Conclusion\n",
    "The observed improvements in episode length and rewards throughout the training iterations demonstrate the effectiveness of the PPO algorithm and an MLP policy in mastering the CartPole-v1 task. This project not only showcases the capabilities of contemporary reinforcement learning techniques but also lays the groundwork for addressing more complex environments and challenges within the domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e37a28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
